{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 1: Manipulação de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (3993656162.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    .master(\"local[8]\") \\  # Usa 2 núcleos de CPU\u001b[0m\n\u001b[1;37m                                                 \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[8]\")\\  # Usa 2 núcleos de CPU\n",
    "    .appName(\"Inter Test\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 34, \"Data Scientist\"),\n",
    "        (\"Bob\", 45, \"Data Engineer\"),\n",
    "        (\"Cathy\", 29, \"Data Analyst\"),\n",
    "        (\"David\", 35, \"Data Scientist\")]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df.select(\"Name\", \"Age\").filter(col(\"Age\") > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|David| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado = df.groupBy(\"Occupation\").avg(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|    Occupation|avg(Age)|\n",
      "+--------------+--------+\n",
      "|Data Scientist|    34.5|\n",
      "| Data Engineer|    45.0|\n",
      "|  Data Analyst|    29.0|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agrupado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado_ordenado = df_agrupado.orderBy(col(\"avg(Age)\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|    Occupation|avg(Age)|\n",
      "+--------------+--------+\n",
      "| Data Engineer|    45.0|\n",
      "|Data Scientist|    34.5|\n",
      "|  Data Analyst|    29.0|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agrupado_ordenado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 2: Funções Avançadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ano_categorizado(age):\n",
    "    if age < 30:\n",
    "        return \"Jovem\"\n",
    "    elif 30 <= age <= 40:\n",
    "        return \"Adulto\"\n",
    "    else:\n",
    "        return \"Senior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_categorizado_udf = udf(ano_categorizado, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_por_categoria = df.withColumn(\"AnoCategorizado\", ano_categorizado_udf(col(\"Age\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+---------------+\n",
      "| Name|Age|    Occupation|AnoCategorizado|\n",
      "+-----+---+--------------+---------------+\n",
      "|Alice| 34|Data Scientist|         Adulto|\n",
      "|  Bob| 45| Data Engineer|         Senior|\n",
      "|Cathy| 29|  Data Analyst|          Jovem|\n",
      "|David| 35|Data Scientist|         Adulto|\n",
      "+-----+---+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_por_categoria.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"Occupation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_diff = df.withColumn(\"AgeDiff\", col(\"Age\") - avg(\"Age\").over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+-------+\n",
      "| Name|Age|    Occupation|AgeDiff|\n",
      "+-----+---+--------------+-------+\n",
      "|Cathy| 29|  Data Analyst|    0.0|\n",
      "|  Bob| 45| Data Engineer|    0.0|\n",
      "|Alice| 34|Data Scientist|   -0.5|\n",
      "|David| 35|Data Scientist|    0.5|\n",
      "+-----+---+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 3: Performance e Otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O particionamento segmenta os dados em frações menores, permitindo uma otimização das operações distribuídas em clusters. Por exemplo, ao dividir um DataFrame com base numa coluna, como \"Occupation\", podemos aceder e processar apenas as partições que são pertinentes em consultas futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partiicionado = df.repartition(4, col(\"Occupation\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partiicionado.write.partitionBy(\"Occupation\").parquet(\"output_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Broadcast Join envia pequenos DataFrames para todos os nós, facilitando a junção sem a necessidade de redistribuir dados, o que torna o processo mais eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = spark.createDataFrame([(\"Data Scientist\", \"Tech\")], [\"Occupation\", \"Industry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df.join(broadcast(small_df), \"Occupation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---+--------+\n",
      "|    Occupation| Name|Age|Industry|\n",
      "+--------------+-----+---+--------+\n",
      "|Data Scientist|Alice| 34|    Tech|\n",
      "|Data Scientist|David| 35|    Tech|\n",
      "+--------------+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 4: Integração com Outras Tecnologias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"c:/Users/lacer/teste_act_inter/df_output.csv\"\n",
    "\n",
    "df.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.csv(output_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|David| 35|Data Scientist|\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|  Bob| 45| Data Engineer|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PySpark se une ao Hadoop HDFS para efetuar leitura e gravação eficientes de dados. Pois o Spark é capaz de ler e gravar dados diretamente em sistemas de arquivos distribuídos, como o HDFS, sem precisar mover os dados para o armazenamento local do nó Spark. Isso torna possível executar operações de E/S em uma máquina muito grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdfs = spark.read.csv(\"hdfs://namenode:8020/path_to_csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdfs.write.csv(\"hdfs://namenode:8020/output_path.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 5: Problema de Caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de log criado: log_file.csv\n"
     ]
    }
   ],
   "source": [
    "# Codigo para criar log de exemplo para problema de caso\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Gerar dados fictícios\n",
    "np.random.seed(0)  # Para reprodutibilidade\n",
    "n_rows = 5000  # Ajuste o número de linhas conforme necessário\n",
    "start_date = datetime.now() - timedelta(days=30)\n",
    "\n",
    "timestamps = [start_date + timedelta(minutes=np.random.randint(0, 43200)) for _ in range(n_rows)]  # 30 dias\n",
    "user_ids = np.random.randint(1, 101, size=n_rows)  # IDs de usuário de 1 a 100\n",
    "actions = np.random.choice(['login', 'logout', 'purchase', 'view'], size=n_rows)\n",
    "\n",
    "# Criar DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'user_id': user_ids,\n",
    "    'action': actions\n",
    "})\n",
    "\n",
    "# Salvar em um arquivo CSV\n",
    "file_path = 'log_file.csv'\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Arquivo de log criado: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'log_file.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_action_counts = df['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_users = user_action_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_users.to_csv('top_10_users.csv', header=['action_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função pd.read_csv(file_path) é utilizada para carregar o ficheiro de log num DataFrame. Para contabilizar as ações, df['user_id'].value_counts() realiza a contagem do número de ações por cada utilizador. Para identificar os 10 utilizadores mais ativos, usa-se user_action_counts.head(10). Por fim, para gravar os dados num ficheiro CSV, aplica-se top_10_users.to_csv('top_10_users.csv', header=['action_count']). Assegure-se de modificar o caminho do ficheiro de log (file_path) e o nome do ficheiro CSV que será gerado (top_10_users.csv) conforme necessário."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
